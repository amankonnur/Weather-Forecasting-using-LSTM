# -*- coding: utf-8 -*-
"""gee&lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDX9AzOjWS-fQMek-3Tf7rb9bacr-80w
"""

!pip install earthengine-api
!earthengine authenticate

import ee
ee.Authenticate()
ee.Initialize(project='my-project-ai-409314')

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline

import pandas as pd
import requests
from io import StringIO

# Import, authenticate and initialize the Earth Engine library.
import ee
try:
    ee.Initialize()
except Exception as e:
    ee.Authenticate()
    ee.Initialize()

# Tensorflow setup.
import tensorflow as tf
from tensorflow import keras
from keras import models
from keras import optimizers
from keras import layers
from keras import callbacks
from keras import backend as K

# used to help transform data
from sklearn import preprocessing

print(f"Using TensorFlow version {tf.__version__}")

# for visualization
import folium

# change to your prefered study period
# must be within the time range of available observed data
# using ~30 years worth of data
START_DATE = "1990-07-01"
END_DATE = "2024-04-22"

# check the number of days between the START_DATE and END_DATE
from datetime import datetime
a = datetime.strptime(START_DATE, "%Y-%m-%d")
b = datetime.strptime(END_DATE, "%Y-%m-%d")

delta = b - a

print(delta.days + 1) # includes END_DATE

# change to the site number of interest
siteNumber = "02387500"

usgs_water_data_url = "https://waterdata.usgs.gov/nwis/dv"
params = {
    "site_no": siteNumber,
    "begin_date": START_DATE,
    "end_date": END_DATE,
    "format": "rdb", # default parameter, do not change
    # 00060 - Discharge, cubic feet per second (Mean)
    "parameter_cd": "00060", # Learn more https://waterdata.usgs.gov/nwis/?tab_delimited_format_info
}

# send request
response = requests.get(usgs_water_data_url, params=params)

if response.status_code == 200:
    print("Successful Data Request! ðŸŽ‰")
else:
    print(response.status_code)
    print(response.reason)
    print("Uh-oh...something went wrong.")

# split the response text into the header information and table data
header, table = response.text.split("# \n")

print(header)

table

col_names = ["agency", "site", "datetime", "discharge", "quality"]
df = pd.read_csv(StringIO(table), sep="\t")
df = df.iloc[1:]
df.columns = col_names

print(f"Table dimensions: {df.shape}")
df

df = df.filter(["datetime", "discharge"], axis=1)
df["discharge"] = df["discharge"].astype(float) * 0.0283168 # convert to m^3/s
df.index = pd.to_datetime(df["datetime"])
df.drop("datetime", axis=1, inplace=True)
df

# specify where the gauge is located so we can filter the basin by location
gauge_lat, gauge_lon = 13.0178, 74.9691
gauge_geom = ee.Geometry.Point([gauge_lon, gauge_lat])

hydrosheds = ee.FeatureCollection("WWF/HydroSHEDS/v1/Basins/hybas_8")

basin = hydrosheds.filterBounds(gauge_geom)

basinOutline = ee.Image().byte()\
    .paint(featureCollection=basin,
           color=1,
           width=3).getMapId()

map = folium.Map(location=[gauge_lat, gauge_lon], zoom_start=9, height=700)
folium.TileLayer(
    tiles=basinOutline["tile_fetcher"].url_format,
    attr="Google Earth Engine",
    overlay=True,
    name="River Basin",
  ).add_to(map)

folium.Marker([gauge_lat, gauge_lon]).add_to(map)

map.add_child(folium.LayerControl())
map

# specify band names we want
met_data = ["total_precipitation", "minimum_2m_air_temperature", "maximum_2m_air_temperature", "u_component_of_wind_10m", "v_component_of_wind_10m", "surface_pressure"]

# filter the collection by date and select the bands on interest
era5 = ee.ImageCollection("ECMWF/ERA5/DAILY")\
    .filterDate(START_DATE, ee.Date(END_DATE).advance(1, "day"))\
    .select(met_data)

scale = 30000 # 27830 # request scale in meters
lumped_forcings = era5.getRegion(basin, scale).getInfo()

# columns.
lumped_forcings[0]

# sample data
lumped_forcings[1]

forcing_df = pd.DataFrame(lumped_forcings[1:])
forcing_df.columns = lumped_forcings[0]
print(forcing_df.shape)
forcing_df.head()

forcing_df = forcing_df.groupby(by="id")\
    .agg({"total_precipitation": "mean",
          "minimum_2m_air_temperature": "mean",
          "maximum_2m_air_temperature": "mean",
          "u_component_of_wind_10m": "mean",
          "v_component_of_wind_10m": "mean",
          "surface_pressure": "mean",
          "time": "mean",
         })

forcing_df.index = pd.to_datetime(forcing_df["time"] * 1e6)
forcing_df.index.name = "datetime"
forcing_df.drop(["time"], axis=1, inplace=True)

new_cols = ["precip", "tmin", "tmax", "uwind", "vwind", "psurface"]
forcing_df.columns = new_cols
print(forcing_df.shape)
forcing_df.head()

model_df = pd.concat([df, forcing_df], axis=1)
model_df.dropna(inplace=True)

print(model_df.shape)

model_df.head()

def lstm_data_prep(data_frame, feature_names, label_names, time_lag=10, pred_period=0, scaling_func=None, pct_train=0.80):
    """
    Prepare data for input into an LSTM model.

    Args:
        data_frame (DataFrame): A pandas DataFrame containing features and labels.
        feature_names (list): List of column names that will be used as input features.
        label_names (list): List of column names that will be used as output labels.

    Kwargs:
        time_lag (int): Time to lag datasets. Default is 10 periods.
        pred_period (int): Time period as forecast outputs. Default is 0 (next day forecast).
        scaling_func (sklearn.preprocessing function): Function to preprocess features.
        pct_train (float): Percent of data to be used for training. Used to prevent scaling on all features.

    Returns:
        X_train (numpy array): Array of training features.
        X_test (numpy array): Array of testing features.
        y_train (numpy array): Array of training labels.
        y_test (numpy array): Array of testing labels.
    """
    # Get features
    X = data_frame[feature_names].values

    # Get the labels
    if pred_period > 0:
        y = data_frame[label_names][time_lag: -pred_period].values
    else:
        y = data_frame[label_names][time_lag:].values

    n_train = int(pct_train * X.shape[0])

    # If a scaling function is provided, scale the data
    if scaling_func is not None:
        scaler = scaling_func.fit(X[:n_train])
        X_scaled = scaler.transform(X)
    else:
        X_scaled = X

    x_shape = [y.shape[0], time_lag, X.shape[1]]
    y_shape = [y.shape[0], pred_period] if pred_period > 0 else [y.shape[0], 1]
    out_X = np.zeros(x_shape)
    out_y = np.zeros(y_shape)

    for i in range(y.shape[0] - pred_period):
        # index tracker for X
        v = time_lag + i if i > 0 else time_lag
        # index tracker for y
        u = pred_period + i if pred_period > 0 else i + 1

        out_X[i, :, :] = X_scaled[i:v, :]
        out_y[i, :] = y[i : u][0] # from (shape, 1) to (shape,)

    n_train = int(pct_train * out_X.shape[0])

    X_train, X_test = out_X[:n_train, :, :], out_X[n_train:, :, :]
    y_train, y_test = out_y[:n_train, :], out_y[n_train:, :]

    return X_train, X_test, y_train, y_test

time_days = 365
feature_columns = ["precip", "tmin", "tmax", "uwind", "vwind", "psurface"]
label_columns = ["discharge"]
pct_train = 0.85

scaler = preprocessing.RobustScaler()
# scaler = preprocessing.MinMaxScaler()

X_train, X_test, y_train, y_test = lstm_data_prep(model_df, feature_columns, label_columns, time_lag=time_days, scaling_func=scaler, pct_train=pct_train)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

def build_model(input_shape, output_shape, nodes=64, optimizer="adam", loss="mse"):
    """
    Build an LSTM model.

    Args:
        input_shape (list or tuple): Input shape for the model, representing [time, features].
        output_shape (int): Output shape for the model, representing [output_shape].

    Kwargs:
        nodes (int): Number of nodes to use in LSTM layers. Default is 64.
        optimizer (str or optimizer): Optimizer to use for the model. Default is Adam optimizer.
        loss (str): Loss function to use for the model. Default is mean squared error (MSE).

    Returns:
        model (keras.Model): Compiled Keras model ready for training.
    """
    inputs = layers.Input(input_shape, name="input_layer")

    x = layers.LSTM(input_shape[1], return_sequences=True, name="lstm_layer")(inputs)

    skip_output = layers.Add()([inputs, x])

    X = layers.LSTM(nodes, return_sequences=False, dropout=0.2)(skip_output)  # recurrent_dropout=0.2

    outputs = layers.Dense(output_shape, activation="linear", name="output_layer")(X)

    model = models.Model(inputs=[inputs], outputs=[outputs], name="lstm_hm")

    model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=["mape", "mae", "mse"])

    return model

def nse_loss(y_true, y_pred):
    """
    Calculate the Nash-Sutcliffe model efficiency coefficient (NSE).

    From: https://en.wikipedia.org/wiki/Nash%E2%80%93Sutcliffe_model_efficiency_coefficient
    NSE is commonly used in hydrology to evaluate model performance (NSE > 0.7 is considered good).

    Args:
        y_true (tf.Tensor): Tensor with true values from observations/labels.
        y_pred (tf.Tensor): Tensor of predicted values from the model.

    Returns:
       tf.Tensor: Inverted NSE value (lower values are better).
    """
    numerator = K.sum(K.pow(y_true - y_pred, 2))
    denominator = K.sum(K.pow(y_true - K.mean(y_true), 2)) + K.epsilon()
    nse = 1 - (numerator / denominator)
    return -1 * nse

inshape = time_days, len(feature_columns)
outshape = 1

EPOCHS = 50
STEPS_PER_EPOCH = 280
BATCH_SIZE = X_train.shape[0]//STEPS_PER_EPOCH
print(f"BATCH_SIZE: {BATCH_SIZE}")
VAL_SPLIT = 0.1

# clipping value to avoid gradient going inf
optimizer = optimizers.Adam(learning_rate=0.001, clipvalue=1.0)

model_nse = build_model(inshape, outshape, optimizer=optimizer, loss=nse_loss)
keras.utils.plot_model(model_nse, "model_nse.png", show_shapes=True)
model_nse.summary()

training_nse = model_nse.fit(x=X_train,
                             y=y_train,
                             epochs=EPOCHS,
                             batch_size=BATCH_SIZE,
                             validation_split=VAL_SPLIT)

fig, ax = plt.subplots(nrows=3, sharex=True, figsize=(10, 10))

ax[0].plot(training_nse.history["loss"], color="#1f77b4", label="Training Loss")
ax[0].plot(training_nse.history["val_loss"], linestyle=":", marker="o", markersize=3, color="#1f77b4", label="Validation Loss")
ax[0].set_ylabel("Loss", fontsize=12)
ax[0].set_xlabel("Epoch", fontsize=12)
ax[0].legend()



ax[1].plot(training_nse.history["mae"], color="#ff7f0e", label="MAE")
ax[1].plot(training_nse.history["val_mae"], linestyle=":", marker="o", markersize=3, color="#ff7f0e", label="Validation MAE")
ax[1].legend()
ax[1].set_ylabel("Mean Absolute Error", fontsize=12)
ax[1].set_xlabel("Epoch", fontsize=12)


ax[2].plot(training_nse.history["mape"], color="#023020", label="MAPE")
ax[2].plot(training_nse.history["val_mape"], linestyle=":", marker="o", markersize=3, color="#023020", label="Validation MAPE")
ax[2].legend()
ax[2].set_xlabel("Epoch", fontsize=12)
ax[2].set_ylabel("Mean Absolute Percentage Error", fontsize=12)



# Set font size for x and y labels
for label in (ax[0].get_yticklabels() + ax[0].get_xticklabels() +
              ax[1].get_yticklabels() + ax[1].get_xticklabels() +
              ax[2].get_yticklabels() + ax[2].get_xticklabels()):
    label.set_fontsize(12)  # Change this value to your desired font size





plt.legend()
plt.show()

# Print x-axis values for each subplot
for i, axis in enumerate(ax):
    # Get the x-axis ticks and labels
    x_ticks = axis.get_xticks()
    x_tick_labels = [int(tick) for tick in x_ticks]
    # Print the x-axis values for each subplot
    print(f"X values for subplot {i}: {x_tick_labels}")

model_huber = build_model(inshape, outshape, optimizer=optimizer, loss="huber_loss")
keras.utils.plot_model(model_huber, "model_huber.png", show_shapes=True)

# training_huber = model_huber.fit(x=X_train,
#                                 y=y_train,
#                                 epochs=EPOCHS,
#                                 batch_size=BATCH_SIZE,
#                                 validation_split=VAL_SPLIT)

# fig, ax = plt.subplots(nrows=3, sharex=True, figsize=(10, 10))

# ax[0].plot(training_huber.history["loss"], color="#1f77b4", label="Training Loss")
# ax[0].plot(training_huber.history["val_loss"], linestyle=":", marker="o", markersize=3, color="#1f77b4", label="Validation Loss")
# ax[0].set_ylabel("Loss")
# ax[0].legend()

# ax[1].plot(training_huber.history["mae"], color="#ff7f0e", label="MAE")
# ax[1].plot(training_huber.history["val_mae"], linestyle=":", marker="o", markersize=3, color="#ff7f0e", label="Validation MAE")
# ax[1].legend()
# ax[1].set_ylabel("Mean Absolute Error")


# ax[2].plot(training_huber.history["mape"], color="#023020", label="MAPE")
# ax[2].plot(training_huber.history["val_mape"], linestyle=":", marker="o", markersize=3, color="#023020", label="Validation MAPE")
# ax[2].legend()
# ax[2].set_xlabel("Epoch")
# ax[2].set_ylabel("Mean Absolute Percentage Error")

# ax[2].set_xticks(range(1, len(training_huber.epoch)+1, 4))
# ax[2].set_xticklabels(range(1, len(training_huber.epoch)+1, 4))

# plt.legend()
# plt.show()

# apply the prediction
y_pred_nse = model_nse.predict(X_test)
y_pred_huber = model_huber.predict(X_test)

# drop the extra dimension for the prediction and test arrays
# this is done for plotting
y_pred_nse = np.squeeze(y_pred_nse)
y_pred_huber = np.squeeze(y_pred_huber)
y_test = np.squeeze(y_test)

# calculate RMSE and nse to see how well are predictions are doing
rmse_nse = np.mean(np.sqrt(np.power((y_test - y_pred_nse), 2)))
rmse_huber = np.mean(np.sqrt(np.power((y_test - y_pred_huber), 2)))
nse_nse = 1 - (np.sum(np.power((y_test - y_pred_nse), 2)) / np.sum(np.power((y_test - y_test.mean()), 2)))
nse_huber = 1 - (np.sum(np.power((y_test - y_pred_huber), 2)) / np.sum(np.power((y_test - y_test.mean()), 2)))

# print(f" >> (nse_loss) NSE: {nse_nse:.4f}  RMSE: {rmse_nse:.4f}")
# print(f" >> (huber_loss) NSE: {nse_huber:.4f}  RMSE: {rmse_huber:.4f}")

# eval_dates = model_df.iloc[X_train.shape[0] + time_days :].index

# fig = plt.figure(figsize=(25, 16))
# gs = fig.add_gridspec(2, 3)

# ax1 = fig.add_subplot(gs[0, :2])
# ax1.plot(eval_dates, y_pred_nse, label="Predicted (nse_loss)")
# ax1.plot(eval_dates, y_test, label="Observed")
# ax1.set_xlabel("Date")
# ax1.set_ylabel("Discharge")
# ax1.legend(fontsize=12)

# ax2 = fig.add_subplot(gs[0, 2:])
# ax2.plot(y_pred_nse, y_test, "o", alpha=0.4)
# ax2.plot([0,y_test.max()], [0,y_test.max()], "k--", alpha=0.75)
# ax2.set_xlabel("Predicted (NSE)")
# ax2.set_ylabel("Observed")


# ax3 = fig.add_subplot(gs[1, :2])
# ax3.plot(eval_dates, y_pred_huber, label="Predicted (huber_loss)")
# ax3.plot(eval_dates, y_test, label="Observed")
# ax3.set_xlabel("Date")
# ax3.set_ylabel("Discharge")
# ax3.legend(fontsize=12)

# ax4 = fig.add_subplot(gs[1, 2:])
# ax4.plot(y_pred_huber, y_test, "o", alpha=0.4)
# ax4.plot([0,y_test.max()], [0,y_test.max()], "k--", alpha=0.75)
# ax4.set_xlabel("Predicted (huber_loss)")
# ax4.set_ylabel("Observed")

# plt.show()

# ... your existing code ...

eval_dates = model_df.iloc[X_train.shape[0] + time_days :].index

fig = plt.figure(figsize=(25, 20))
gs = fig.add_gridspec(2, 3)

ax1 = fig.add_subplot(gs[0, :2])
ax1.plot(eval_dates, y_pred_nse, label="Predicted (nse_loss)")
ax1.plot(eval_dates, y_test, label="Observed")
ax1.set_xlabel("Date",fontsize=20)
ax1.set_ylabel("Forecast", fontsize=20)  # Corrected the function name
ax1.legend( )

# ... the rest of your code ...

# Set font size for x and y labels in ax1
for label in (ax1.get_yticklabels() + ax1.get_xticklabels()):
    label.set_fontsize(20)  # Change this value to your desired font size


ax2 = fig.add_subplot(gs[0, 2:])
ax2.plot(y_pred_nse, y_test, "o", alpha=0.4)
ax2.plot([0,y_test.max()], [0,y_test.max()], "k--", alpha=0.75)
ax2.set_xlabel("Predicted (NSE)", fontsize=20)
ax2.set_ylabel("Observed",fontsize=20)

# Set font size for x and y labels in ax1
for label in (ax2.get_yticklabels() + ax2.get_xticklabels()):
    label.set_fontsize(20)  # Change this value to your desired font size

# Similar font size and format changes for other axes (ax3 and ax4)

ax3 = fig.add_subplot(gs[1, :2])
ax3.plot(eval_dates, y_pred_huber, label="Predicted (huber_loss)")
ax3.plot(eval_dates, y_test, label="Observed")
ax3.set_xlabel("Date",fontsize=35)
ax3.set_ylabel("Forecast",fontsize=35)
ax3.legend()

for label in (ax3.get_yticklabels() + ax3.get_xticklabels()):
    label.set_fontsize(20)

ax4 = fig.add_subplot(gs[1, 2:])
ax4.plot(y_pred_huber, y_test, "o", alpha=0.4)
ax4.plot([0,y_test.max()], [0,y_test.max()], "k--", alpha=0.75)
ax4.set_xlabel("Predicted (huber_loss)",fontsize=20)
ax4.set_ylabel("Observed",fontsize=20)


for label in (ax4.get_yticklabels() + ax4.get_xticklabels()):
    label.set_fontsize(20)
# ... repeat for ax3 and ax4 ...

plt.show()

feature_columns = ["precip", "tmin", "tmax", "uwind", "vwind", "psurface"]
label_columns = ["discharge"]
pct_train = 0.85

time_lag = 120
pred_period = 5

# scaler = preprocessing.MinMaxScaler()
scaler = preprocessing.RobustScaler()


X_train, X_test, y_train, y_test = lstm_data_prep(model_df, feature_columns, label_columns, time_lag=time_lag, pred_period=pred_period, scaling_func=scaler, pct_train=pct_train)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

inshape = time_lag, len(feature_columns)
outshape = pred_period

EPOCHS = 50
STEPS_PER_EPOCH = 500
BATCH_SIZE = X_train.shape[0]//STEPS_PER_EPOCH
print(f"BATCH_SIZE: {BATCH_SIZE}")
VAL_SPLIT = 0.1

# clipping value to avoid gradient going inf
optimizer = optimizers.Adam(learning_rate=0.001, clipvalue=1.0)

forecast_model = build_model(inshape, outshape, optimizer=optimizer, loss="huber_loss")
keras.utils.plot_model(forecast_model, "forecast_model.png", show_shapes=True)
forecast_model.summary()

forecast_training = forecast_model.fit(x=X_train,
                                       y=y_train,
                                       epochs=EPOCHS,
                                       batch_size=BATCH_SIZE,
                                       validation_split=VAL_SPLIT)

# run the predictions
forecast = forecast_model.predict(X_test)
 # print the shape (sanity check)n
forecast.shape

eval_dates = model_df.iloc[X_train.shape[0] + time_lag:].index
print(f"start_date: {eval_dates[:1][0]}, end_date: {eval_dates[-1:][0]}")

from ipywidgets import *
import datetime

eval_dates = model_df.iloc[X_train.shape[0] + time_lag:].index
init_date = datetime.datetime(2020, 5, 20)

@widgets.interact(date=DatePicker(description="Pick a Date:", value=init_date))
def plotUpdate(date):
    index_eval = int(np.where(eval_dates == str(date))[0])
    fig, ax = plt.subplots(figsize=(15, 8))
    historic = ax.plot(eval_dates[index_eval - pred_period*3: index_eval], y_test[index_eval - pred_period*3: index_eval, 0], label="historic", alpha=0.5)
    predicted = ax.plot(eval_dates[index_eval: index_eval + pred_period], forecast[index_eval, :], color="C1", marker="o", label="predicted")
    truth = ax.plot(eval_dates[index_eval: index_eval + pred_period], y_test[index_eval: index_eval + pred_period, 0], color="C0", marker="o",label="truth")
    ax.set_title(f"{pred_period} Days Forecast for {date}", fontsize=12)
    ax.set_ylabel("Prediction [m^m]")
    plt.legend(fontsize=12)
    plt.show()

X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1]))

def build_ann_model(X_train, output_shape=1, nodes=64, optimizer="adam", loss="mse"):
    """
    Build an ANN model.

    Args:
        X_train (np.ndarray): Input data for training.
        output_shape (int): Output shape for the model.

    Kwargs:
        nodes (int): Number of nodes to use in hidden layers. Default is 64.
        optimizer (str or optimizer): Optimizer to use for the model. Default is Adam optimizer.
        loss (str): Loss function to use for the model. Default is mean squared error (MSE).

    Returns:
        model (keras.Model): Compiled Keras model ready for training.
    """
    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1]))

    model = models.Sequential([
        layers.Dense(nodes, activation="relu", input_shape=X_train_reshaped.shape[1:]),
        layers.Dense(nodes, activation="relu"),
        layers.Dense(output_shape, activation="linear")
    ])

    model.compile(optimizer=optimizer, loss=loss, metrics=["mae", "mse"])

    return model

# Define the output shape
outshape = y_train.shape[1]

# Reshape the input data for the ANN model
X_train_reshaped = X_train.reshape(X_train.shape[0], -1)
X_test_reshaped = X_test.reshape(X_test.shape[0], -1)

# Build and train the ANN model
ann_model = build_ann_model(input_shape=(X_train_reshaped.shape[1],), output_shape=outshape)
ann_training = ann_model.fit(x=X_train_reshaped, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VAL_SPLIT)

# Evaluate model performance
ann_scores = ann_model.evaluate(X_test_reshaped, y_test)

# Compare results
print("ANN Model Performance:")
print("Mean Squared Error:", ann_scores[1])
print("Mean Absolute Error:", ann_scores[2])
# Add more metrics if needed

def mean_absolute_error(y_true, y_pred):
  # Avoid division by zero
  return np.mean(np.abs(y_true - y_pred) / np.maximum(np.ones_like(y_pred), np.abs(y_true)))

print("LSTM Model Performance:")
print("Mean Squared Error:", training_nse.history["val_mse"][-1])
print("Mean Absolute Error:", training_nse.history["val_mae"][-1])
# Add more metrics if needed

# Visualize results (if needed)
# Plot MSE and MAE for both models over epochs

plt.plot(training_nse.history['val_mse'], label='LSTM Val MSE',color='blue')
plt.plot(training_nse.history['val_mae'], label='ANN Val MSE',color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Define and train the LSTM model with Huber loss
model_huber = build_model(inshape, outshape, optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss="huber_loss")  # Use Adam optimizer directly
training_huber = model_huber.fit(x=X_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VAL_SPLIT)

# Plot MAE for LSTM model with NSE loss
plt.plot(training_nse.history['val_mae'], label='LSTM (nse_loss) Validation MAE', color='blue')

# Plot MAE for LSTM model with Huber loss

# Plot MAE for ANN model
plt.plot(training_huber.history['val_mae'], label='ANN Validation MAE', color='red')

# Plot MAE for ANN model
#plt.plot(ann_training.history['val_mae'], label='ANN Validation MAE', color='green')

plt.title('Validation MAE Comparison')
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.legend()
plt.show()

import datetime

import numpy as np

eval_dates = np.array(['2018-05-01', '2018-05-02', '2018-05-03', ...])

# Find the index of the date in the evaluation dates array
index = np.where(eval_dates == date_obj)[0][0]

# Find the index of the date in the evaluation dates array
index = np.where(eval_dates == date_obj)[0][0]

# Specify the date for which you want to print the forecast
date_to_predict = "2018-05-01"  # Example date

# Convert the date to a datetime object
date_obj = datetime.datetime.strptime(date_to_predict, "%Y-%m-%d")

# Find the index of the date in the evaluation dates array
index = np.where(eval_dates == date_obj)[0][0]

# Predict the discharge for the specified date using the forecast model
forecast = forecast_model.predict(X_test)

# Get the forecast values for the specified date
forecast_values = forecast[index]

#print(f"Forecast values for {date_to_predict}: {forecast_values}")
print("Accuracy of ANN:",training_nse.history["val_mae"][-1])
print("Accuracy of LSTM:",forecast_values)